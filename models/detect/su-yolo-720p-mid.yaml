# SU-YOLO 720p Mid — balanced accuracy/speed for pedestrian detection
#
# Design: between nano-v2 (7 GOPS) and full (40+ GOPS)
#   - 3 detection scales: P2/P3/P4 (P4 catches medium objects like pedestrians)
#   - Moderate channels: 32→64→128 (between nano 24→96 and full 64→512)
#   - 64ch head outputs
#   - reg_max: 8, time_step: 1
#
# GOPS estimate at 1280x736, time_step=1:
#   SEncoderLite (3→32):  1.28 GOPS  (stem)
#   BasicBlock1 (32→64):  0.86 GOPS  (P2/4)
#   BasicBlock1 (64→128): 0.86 GOPS  (P3/8)
#   BasicBlock1 (128→128):0.43 GOPS  (P4/16)
#   Head (PAN 3-scale):   3.50 GOPS  (neck)
#   SDDetect (P2+P3+P4):  8.00 GOPS  (heads)
#   TOTAL:               ~15 GOPS
#
# ZCU102 deployment:
#   Vitis AI DPU (60% util):  ~45-50 FPS
#   HLS-optimized (80% util): ~60-65 FPS ✓
#
# Train: python train.py --cfg models/detect/su-yolo-720p-mid.yaml \
#          --data data/visdrone-mot.yaml --hyp data/hyps/hyp.visdrone.yaml \
#          --img 1280 --batch 4 --time-step 1 --epochs 200 --cos-lr
# Export: python export.py --weights best.pt --imgsz 736 1280 --include vitisai

# parameters
nc: 10  # VisDrone classes (overridden by data yaml at runtime)
depth_multiple: 1.0
width_multiple: 1.0  # explicit channels below — do NOT scale
reg_max: 8  # lighter bbox regression (default 16)

# anchors
anchors: 2

# backbone — 3-stage (P2+P3+P4). Moderate channels.
backbone:
  [
   # SEncoderLite: stride-2 first → halves spatial dims cheaply (Cin=3)
   [-1, 1, SEncoderLite, [32, 3, 2]],  # 0-P1/2  32ch, 640x368

   # elan-1 block: 32→64ch, stride 2
   [-1, 1, BasicBlock1, [64, 64, 32, 1]],  # 1-P2/4  64ch, 320x184

   # elan-1 block: 64→128ch, stride 2
   [-1, 1, BasicBlock1, [128, 128, 64, 1]],  # 2-P3/8  128ch, 160x92

   # elan-1 block: 128→128ch, stride 2
   [-1, 1, BasicBlock1, [128, 128, 64, 1]],  # 3-P4/16  128ch, 80x46
  ]

# head — P2/P3/P4 three-scale PAN + detect
head:
  [
    # 1x1 channel reduce on P4
   [ -1, 1, SConv, [ 64, 1, 1 ] ],  # 4 — 128→64ch @ 80x46

    # upsample + concat with backbone P3
   [ -1, 1, SUpsample, [ 2 ] ],  # 5 — 64ch @ 160x92
   [ [ -1, 2 ], 1, SConcat, [ 1 ] ],  # 6 — cat → 192ch @ 160x92

    # elan-2 block for P3
   [ -1, 1, BasicBlock2, [ 64, 64, 32, 1 ] ],  # 7 (P3/8-medium) 64ch

    # upsample + concat with backbone P2
   [ -1, 1, SUpsample, [ 2 ] ],  # 8 — 64ch @ 320x184
   [ [ -1, 1 ], 1, SConcat, [ 1 ] ],  # 9 — cat → 128ch @ 320x184

    # elan-2 block for P2 output
   [ -1, 1, BasicBlock2, [ 64, 64, 32, 1 ] ],  # 10 (P2/4-small) 64ch

    # downsample + concat with head P3
   [-1, 1, SConv, [64, 3, 2]],  # 11 — 64ch @ 160x92
   [ [ -1, 7 ], 1, SConcat, [ 1 ] ],  # 12 — cat → 128ch @ 160x92

    # elan-2 block for P3 output
   [ -1, 1, BasicBlock2, [ 64, 64, 32, 1 ] ],  # 13 (P3/8-medium) 64ch

    # downsample + concat with head P4
   [-1, 1, SConv, [64, 3, 2]],  # 14 — 64ch @ 80x46
   [ [ -1, 4 ], 1, SConcat, [ 1 ] ],  # 15 — cat → 128ch @ 80x46

    # elan-2 block for P4 output
   [ -1, 1, BasicBlock2, [ 64, 64, 32, 1 ] ],  # 16 (P4/16-large) 64ch

    # detect at P2 + P3 + P4 (3 scales)
   [ [ 10, 13, 16 ], 1, SDDetect, [ nc ] ],  # SDDetect(P2, P3, P4)
  ]
